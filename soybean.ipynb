{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://data.mendeley.com/archiver/3fmjm7ncc6?version=2 -O soybean.zip\n",
    "!unzip soybean.zip\n",
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR='dataset/'\n",
    "classes=os.listdir(DATASET_DIR)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for cname in classes:\n",
    "  path=os.path.join(DATASET_DIR,cname)\n",
    "  imgs=os.listdir(path)\n",
    "  data.extend([[os.path.join(path,imgname),cname] for imgname in imgs])\n",
    "  print(\"{} : {}\".format(cname,len(imgs)))\n",
    "\n",
    "data=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "X=np.array([cv2.resize(cv2.imread(imgname),(100,100)).reshape(-1) for imgname in data[:,0]])\n",
    "y=data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(train):\n",
    "    r, c = 3,5\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            img=train[i*c+j].reshape(100,100,3)\n",
    "            #img=cv2.imread(img_path)\n",
    "            axs[i,j].imshow(img)\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(image, angle=90, scale=1.0):\n",
    "    w = image.shape[1]\n",
    "    h = image.shape[0]\n",
    "    M = cv2.getRotationMatrix2D((w/2,h/2), angle, scale)\n",
    "    image = cv2.warpAffine(image,M,(w,h))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(image, vflip=False, hflip=False):\n",
    "    if hflip or vflip:\n",
    "        if hflip and vflip:\n",
    "            c = -1\n",
    "        else:\n",
    "            c = 0 if vflip else 1\n",
    "        image = cv2.flip(image, flipCode=c)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augment(image):  \n",
    "    aug=np.random.randint(3)\n",
    "    img = image.copy()\n",
    "    if aug ==1:\n",
    "      img = flip(img, vflip=True, hflip=False)\n",
    "    if aug ==2:\n",
    "      img = flip(img, vflip=False, hflip=True)\n",
    "    else:\n",
    "      img = rotate(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug=[]\n",
    "y_aug=[]\n",
    "for img,target in zip(X,y):\n",
    "  X_aug.append(image_augment(img.reshape(100,100,3)).reshape(-1))\n",
    "  y_aug.append(target)\n",
    "\n",
    "X_aug=np.array(X_aug)\n",
    "y_aug=np.array(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.hstack((y,y_aug))\n",
    "X=np.vstack((X,X_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(X_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.get_dummies(y).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \"\"\"docstring for network\"\"\"\n",
    "    def __init__(self, input_shape,layers):\n",
    "        super(network, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.input_shape=input_shape\n",
    "        self.initialize_parameters() \n",
    "        self.training=True\n",
    "\n",
    "    def initialize_linear_layer(self,n_x, n_y):\t\n",
    "        W = np.random.randn(n_y, n_x) * 0.01\n",
    "        b = np.zeros(shape=(n_y, 1))\t\n",
    "\n",
    "        assert(W.shape == (n_y, n_x))\n",
    "        assert(b.shape == (n_y, 1))\n",
    "\n",
    "        return W,b\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        input_dim=self.input_shape\n",
    "        for i,layer  in enumerate(self.layers):\n",
    "            if layer[\"type\"]==\"linear\":\n",
    "                output_dim=layer[\"output\"]\n",
    "                W,b=self.initialize_linear_layer(input_dim,output_dim)\n",
    "                layers[i][\"W\"]=W\n",
    "                layers[i][\"b\"]=b\n",
    "                input_dim=output_dim\n",
    "\n",
    "\n",
    "    def sigmoid(self,Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s\n",
    "\n",
    "    def sigmoid_backward(self,dA, Z):\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA * s * (1-s)\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "\n",
    "    def relu(self,Z):\n",
    "        A = np.maximum(0,Z)\n",
    "        assert(A.shape == Z.shape)\n",
    "        return A\n",
    "\n",
    "    def relu_backward(self,dA, Z):\n",
    "        dZ = np.array(dA, copy=True) \n",
    "        dZ[Z <= 0] = 0\n",
    "        assert (dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "\n",
    "    def softmax(self,x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "    def linear_layer(self,x, W, b, activation):\n",
    "        Z = W.dot(x) + b\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            A= self.sigmoid(Z)\n",
    "        elif activation == \"relu\":\n",
    "            A= self.relu(Z)\n",
    "\n",
    "        assert (A.shape == (W.shape[0], x.shape[1]))\n",
    "\n",
    "        return A,{\"A\":x, \"W\":W, \"b\":b, \"Z\":Z}\t\n",
    "\n",
    "    def linear_backward(self,dAL,grad_info,activation,m):\n",
    "        if activation == \"sigmoid\":\n",
    "            dZ= self.sigmoid_backward(dAL, grad_info[\"Z\"])\n",
    "        elif activation == \"relu\":\n",
    "            dZ= self.relu_backward(dAL, grad_info[\"Z\"])\n",
    "\n",
    "        dW = 1./m * np.dot(dZ,grad_info[\"A\"].T)\n",
    "        db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA_prev = np.dot(grad_info[\"W\"].T,dZ)\n",
    "        assert (dA_prev.shape == grad_info[\"A\"].shape)\n",
    "        assert (dW.shape == grad_info[\"W\"].shape)\n",
    "        assert (db.shape == grad_info[\"b\"].shape)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    \n",
    "    def forward(self,A):\n",
    "        for i,layer  in enumerate(self.layers):\n",
    "            A_prev=A\n",
    "            if layer[\"type\"]==\"linear\":\n",
    "                A,grad_info=self.linear_layer(A_prev, layer[\"W\"], layer[\"b\"], layer[\"activation\"])\n",
    "                if self.training:\n",
    "                    self.layers[i][\"grad_info\"]=grad_info\n",
    "            elif layer[\"type\"]==\"softmax\":\n",
    "                A=self.softmax(A_prev)\n",
    "        return A\n",
    "\n",
    "    def backward(self,AL,Y,learning_rate):\n",
    "\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape) \n",
    "\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            layer=self.layers[i]\n",
    "            grad_info=layer[\"grad_info\"]\n",
    "            dAL, dW, db = self.linear_backward(dAL,grad_info,layer[\"activation\"],m)\n",
    "            layers[i][\"W\"]=layers[i][\"W\"] - learning_rate*dW\n",
    "            layers[i][\"b\"]=layers[i][\"b\"] - learning_rate*db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):    \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost) \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, learning_rate=0.0075, epochs=30,batch_size=256, print_cost=True): #lr was 0.009\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         \n",
    "    steps=0\n",
    "    for i in range(1, epochs+1):\n",
    "      for j in range(0,X.shape[1]//batch_size):\n",
    "          steps+=1\n",
    "          X_batch=X[:,j*batch_size:(j+1)*batch_size]\n",
    "          Y_batch=Y[:,j*batch_size:(j+1)*batch_size]\n",
    "          #print(j,Y_batch.shape)\n",
    "          AL=model.forward(X_batch)\n",
    "          cost = compute_cost(AL, Y_batch)\n",
    "\n",
    "          model.backward(AL,Y_batch,learning_rate)\n",
    "\n",
    "          if print_cost and steps % 100 == 0:\n",
    "              print (\"Cost after iteration %i: %f\" % (steps, cost))\n",
    "              costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[\n",
    "        {\n",
    "        \"type\":\"linear\",\n",
    "        \"output\":1024,\n",
    "        \"activation\":\"relu\",\n",
    "    },\n",
    "\n",
    "        {\n",
    "        \"type\":\"linear\",\n",
    "        \"output\":512,\n",
    "        \"activation\":\"relu\",\n",
    "    },\n",
    "\n",
    "        {\n",
    "        \"type\":\"linear\",\n",
    "        \"output\":4,\n",
    "        \"activation\":\"sigmoid\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=X_train.shape[1]\n",
    "model=network(input_shape,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(X_train.T, y_train.T, learning_rate=0.03,epochs=5,batch_size=128, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y,y_pred):\n",
    "  return np.sum(np.argmax(y_pred,axis=1)==np.argmax(y,axis=1))*100/np.argmax(y,axis=1).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.forward(X_test.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
